(window.webpackJsonp=window.webpackJsonp||[]).push([[42],{735:function(t,a,e){"use strict";e.r(a);var s=e(19),n=Object(s.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("Are you planning to use "),e("a",{attrs:{href:"https://kafka.apache.org/documentation/streams/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kafka Streams"),e("OutboundLink")],1),t._v(" to build a distributed application? This blog post shows some advanced tips and techniques for Kafka Streams developers. Over the last two years, we discovered these techniques to handle advanced Kafka Streams capabilities.")]),t._v(" "),e("p",[t._v("We built "),e("a",{attrs:{href:"https://github.com/kestra-io/kestra",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kestra"),e("OutboundLink")],1),t._v(", an open-source data orchestration and scheduling platform, and we decided to use "),e("a",{attrs:{href:"https://kafka.apache.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kafka"),e("OutboundLink")],1),t._v(" as the central datastore to build a scalable architecture. We rely heavily on Kafka Streams for most of our services (the executor and the scheduler) and have made some assumptions on how it handles the workload.")]),t._v(" "),e("p",[t._v("However, Kafka has some restrictions since it is not a database, so we need to deal with the constraints and adapt the code to make it work with Kafka. We will cover topics such as using the same Kafka topic for source and destination, and creating a custom joiner for Kafka Streams, to ensure high throughput and low latency while adapting to the constraints of Kafka and making it work with Kestra.")]),t._v(" "),e("h2",{attrs:{id:"why-use-apache-kafka"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#why-use-apache-kafka"}},[t._v("#")]),t._v(" Why Use Apache Kafka?")]),t._v(" "),e("p",[t._v("Apache Kafka is an open-source distributed event store and stream-processing platform that handles high volumes of data at high velocity. The Kafka ecosystem also brings a robust streaming framework called Kafka Streams designed to simplify the creation of streaming data pipelines and perform high-level operations like joining and aggregation. One of its key benefits is the ability to embed the streaming application directly within your Java application, eliminating the need to manage a separate platform.")]),t._v(" "),e("p",[t._v("While building Kestra, we wanted to rely only on the queue as a database for our application (persistent queue) without additional dependencies. We analyzed many candidates (RabbitMQ, Apache Pulsar, Redis, etc.) and found that Apache Kafka was the only one that covered everything for our use case.")]),t._v(" "),e("h2",{attrs:{id:"same-kafka-topic-for-source-and-destination"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#same-kafka-topic-for-source-and-destination"}},[t._v("#")]),t._v(" Same Kafka Topic for Source and Destination")]),t._v(" "),e("p",[t._v("In Kestra, we have a "),e("a",{attrs:{href:"https://kafka.apache.org/intro#intro_concepts_and_terms",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kafka topic"),e("OutboundLink")],1),t._v(" for the current flow "),e("a",{attrs:{href:"https://kestra.io/docs/concepts/executions.html#execution",target:"_blank",rel:"noopener noreferrer"}},[t._v("execution"),e("OutboundLink")],1),t._v(". That topic is both the source and the destination. We update the current execution to add some information and send it back to Kafka for further processing.")]),t._v(" "),e("p",[t._v("Initially, we were unsure if this design was possible with Kafka. We "),e("a",{attrs:{href:"https://twitter.com/tchiotludo/status/1252197729406783488",target:"_blank",rel:"noopener noreferrer"}},[t._v("asked"),e("OutboundLink")],1),t._v(" Matthias J. Sax, one of the primary maintainers of Kafka Streams, who responded on "),e("a",{attrs:{href:"https://stackoverflow.com/questions/61316312/does-kafka-stream-with-same-sink-source-topics-with-join-is-supported",target:"_blank",rel:"noopener noreferrer"}},[t._v("Stack Overflow"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[t._v("Yes, it's possible if you are certain that for the same key (the execution ID, in this case), you have only one process that can write it. If you see this warning in the console "),e("code",[t._v("Detected out-of-order KTable update for execution at offset 10, partition 7.")]),t._v(", you likely have more than one process for the same key, which can lead to unexpected behavior (like overwriting previous values).")]),t._v(" "),e("p",[t._v("Struggling to understand what this means? Imagine a topology with the topic as the source, some branching logic, and two different processes writing to the same destination:")]),t._v(" "),e("div",{staticClass:"language-java extra-class"},[e("pre",{pre:!0,attrs:{class:"language-java"}},[e("code",[e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("KStream")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Execution")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" executions "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" builder\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("stream")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"executions"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Consumed")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Execution")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\nexecutions\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("mapValues")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("readOnlyKey"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("to")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"executions"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Produced")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Execution")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\nexecutions\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("leftJoin")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        builder"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("table")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"results"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Consumed")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WorkerTaskResult")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("readOnlyKey"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value1"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value2"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" value1\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("to")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"executions"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Produced")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Execution")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("p",[t._v("In this case, a concurrent process can write this topic on the same key, "),e("strong",[t._v("overwriting the previous value, effectively losing its data")]),t._v(". In this context, you must define a single writer for a key at a given time. This leads us to our next section, a custom joiner.")]),t._v(" "),e("h2",{attrs:{id:"custom-joiner-for-kafka-streams"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#custom-joiner-for-kafka-streams"}},[t._v("#")]),t._v(" Custom Joiner for Kafka Streams")]),t._v(" "),e("p",[t._v("We wrote a microservice to process the executions and split the microservice into multiple topics:")]),t._v(" "),e("ul",[e("li",[t._v("A topic with the executions (with multiple tasks)")]),t._v(" "),e("li",[t._v("A topic with tasks results")])]),t._v(" "),e("p",[t._v("To allow the next task of a flow to start, we need to create a state with all tasks results merged into the current execution. Our first thought was to use "),e("code",[t._v("join()")]),t._v(" from Kafka Streams. In hindsight, this was not a very clever decision. 😉")]),t._v(" "),e("p",[t._v("All joins provided by Kafka Streams were "),e("strong",[t._v("designed with aggregation in mind")]),t._v(", like sum, avg, etc. It processes all the incoming data from both topics 1 to 1. We will see all the changes on the streams on both sides, as illustrated below:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# timeline\n--A-------- > Execution\n-----B--C-- > Task Result\n\n# join result timeline\n- (A,null)\n- (A, B) => emit (A+B)\n- (A, C) => emit (A+C) <=== you have overwritten the result of A+B\n- (A+B, null)\n- (A+C, null) <== we will never have (A+B+C)\n")])])]),e("p",[t._v("However, we are building a "),e("a",{attrs:{href:"https://developer.mozilla.org/en-US/docs/Glossary/State_machine",target:"_blank",rel:"noopener noreferrer"}},[t._v("state machine"),e("OutboundLink")],1),t._v(" and want to keep the "),e("strong",[t._v("last state")]),t._v(" of execution, meaning we do not want to see the intermediate states. In this case, we have no choice but to build a custom joiner since Kafka Streams doesn't have a built-in one.")]),t._v(" "),e("p",[t._v("Our custom joiner needs to:")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/streams/ExecutorFromExecutionTransformer.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("Manually create a store"),e("OutboundLink")],1),t._v(" that will save the last state of an execution.")]),t._v(" "),e("li",[t._v("Create a custom "),e("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/executors/ExecutorMain.java#L216-L246",target:"_blank",rel:"noopener noreferrer"}},[t._v("merge function"),e("OutboundLink")],1),t._v(" that will merge the execution stream with the tasks results stream.")]),t._v(" "),e("li",[e("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/streams/ExecutorJoinerTransformer.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("Get the last value"),e("OutboundLink")],1),t._v(" from the state, add the task result and emit the new state that will finally be saved on the store and the final topic.")])]),t._v(" "),e("p",[t._v("With all this, we make sure that the execution state will always be the last version, whatever the number of tasks results coming in parallel might be.")]),t._v(" "),e("h2",{attrs:{id:"distributed-workload-between-multiple-backends"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#distributed-workload-between-multiple-backends"}},[t._v("#")]),t._v(" Distributed Workload Between Multiple Backends")]),t._v(" "),e("p",[t._v("In Kestra, a scheduler will look up all flows either with scheduled execution or with a long-polling mechanism (detecting files on S3 or SFTP). To avoid a single point of failure on this service, we needed to split the flows between all instances of schedulers.")]),t._v(" "),e("p",[t._v("We rely on Kafka's consumer groups that will handle the complexity of a distributed system for us. Here's how we do it:")]),t._v(" "),e("ul",[e("li",[t._v("Create a "),e("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/KafkaFlowListeners.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kafka stream"),e("OutboundLink")],1),t._v(" that will read in a "),e("code",[t._v("KTable")]),t._v(" and transmit all the results to a "),e("code",[t._v("Consumer")]),t._v(".")]),t._v(" "),e("li",[t._v("Listen to state changes (mostly "),e("code",[t._v("REBALANCED")]),t._v(" streams) and empty all the flows for the "),e("code",[t._v("Consumer")]),t._v(".")]),t._v(" "),e("li",[t._v("On the "),e("code",[t._v("READY")]),t._v(" state, read all the "),e("code",[t._v("KTable")]),t._v(" again.")])]),t._v(" "),e("p",[t._v("With these, all flows will be dispatched to all listeners. That means if you have a thousand flows, every consumer will have ~500 flows (depending on the repartition of keys). Kafka will handle all the heavy parts of the distributed systems, such as:")]),t._v(" "),e("ul",[e("li",[t._v("Heartbeat to detect failure for a consumer")]),t._v(" "),e("li",[t._v("Notifications for rebalancing")]),t._v(" "),e("li",[t._v("Ensure exactly-once semantic for a topic, ensuring that only one consumer will handle the data")])]),t._v(" "),e("p",[t._v("This way, you will have a fully distributed system thanks to Kafka without the pain of going through a "),e("a",{attrs:{href:"https://jepsen.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Jespen"),e("OutboundLink")],1),t._v(" analysis.")]),t._v(" "),e("h2",{attrs:{id:"partitions-to-detect-dead-kafka-consumers"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#partitions-to-detect-dead-kafka-consumers"}},[t._v("#")]),t._v(" Partitions to Detect Dead Kafka Consumers")]),t._v(" "),e("p",[t._v("In Kestra, "),e("a",{attrs:{href:"https://kestra.io/docs/architecture/#worker",target:"_blank",rel:"noopener noreferrer"}},[t._v("workers"),e("OutboundLink")],1),t._v(' are Kafka Consumers that process tasks submitted to it and will handle all the computing (connect and query a database, fetch data from external services, etc.) and are long-running processes. We need to detect when a worker was processing a task and died. The reasons for the process "dying" could range from an outage to a simple restart during processing.')]),t._v(" "),e("p",[t._v("Thanks to the Kafka consumer mechanism, we can know the specific partitions affected by a died consumer. We use these features to detect dead workers:")]),t._v(" "),e("ul",[e("li",[t._v("We create a "),e("code",[t._v("UUID")]),t._v(" on startup for the worker.")]),t._v(" "),e("li",[t._v("When a consumer connects to Kafka, we "),e("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/KafkaWorkerTaskQueue.java#L157-L187",target:"_blank",rel:"noopener noreferrer"}},[t._v("listen to the partitions"),e("OutboundLink")],1),t._v(" affected using a "),e("code",[t._v("ConsumerRebalanceListener")]),t._v(". We publish to Kafka a WorkerInstance with the "),e("code",[t._v("UUID")]),t._v(" and assigned partitions.")]),t._v(" "),e("li",[t._v("For each task run, we publish a "),e("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/KafkaWorkerTaskQueue.java#L112-L123",target:"_blank",rel:"noopener noreferrer"}},[t._v("TaskRunning"),e("OutboundLink")],1),t._v(" message with the worker UUID.")])]),t._v(" "),e("p",[t._v("Now, let's handle the data stored in Kafka. The main logic is a "),e("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/streams/WorkerInstanceTransformer.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kafka Stream"),e("OutboundLink")],1),t._v(" which will:")]),t._v(" "),e("ul",[e("li",[t._v("Create a global "),e("code",[t._v("KTable")]),t._v(" with all the "),e("code",[t._v("WorkerInstance")]),t._v(".")]),t._v(" "),e("li",[t._v("On every change, it will listen to the changed "),e("code",[t._v("WorkerInstance")]),t._v(".")]),t._v(" "),e("li",[t._v("If there is a new "),e("code",[t._v("WorkerInstance")]),t._v(", we look at the partitions assigned to it. If there is an overlap between this instance's partitions and the previous one, we know that the previous "),e("code",[t._v("WorkerInstance")]),t._v(" is dead. In Kafka, you can't have two consumers on the same partition.")]),t._v(" "),e("li",[t._v("We only need to look at the affected tasks in this "),e("code",[t._v("WorkerInstance")]),t._v(" and resend them for processing.")])]),t._v(" "),e("p",[t._v("Et voilà! We have detection of dead consumers using just the Kafka API. 🎉")]),t._v(" "),e("h2",{attrs:{id:"beware-of-state-store-all"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#beware-of-state-store-all"}},[t._v("#")]),t._v(" Beware of State Store "),e("code",[t._v("all()")])]),t._v(" "),e("p",[t._v("We use a "),e("a",{attrs:{href:"https://kafka.apache.org/31/documentation/streams/developer-guide/dsl-api.html#streams_concepts_globalktable",target:"_blank",rel:"noopener noreferrer"}},[t._v("GlobalKTable"),e("OutboundLink")],1),t._v(" to detect "),e("RouterLink",{attrs:{to:"/docs/developer-guide/triggers/flow.html"}},[t._v("flow triggers")]),t._v(". For all the flows on the cluster, we test all the flow's "),e("RouterLink",{attrs:{to:"/docs/developer-guide/conditions/"}},[t._v("conditions")]),t._v(" to find matching flows. For this, we are using an API to fetch all flows from a "),e("code",[t._v("GlobalKTable")]),t._v(" using "),e("code",[t._v("store.all()")]),t._v(" that returns all the flows from RocksDB (internal database from Kafka Stream).")],1),t._v(" "),e("p",[t._v("Our first assumption was that "),e("code",[t._v("all()")]),t._v(" returns an object (Flow in our case), as the API return Object, but we discovered that the "),e("code",[t._v("all()")]),t._v(" method will:")]),t._v(" "),e("ul",[e("li",[t._v("Fetch all the data from RocksDB")]),t._v(" "),e("li",[t._v("Deserialize the data from RocksDB that is stored as byte, and map it to concrete Java POJO")])]),t._v(" "),e("p",[t._v("So each time we call the "),e("code",[t._v("all()")]),t._v(" method, all values are deserialized, which can lead to high CPU usage and latency on your stream. We are talking about all "),e("a",{attrs:{href:"https://kestra.io/docs/concepts/flows.html#revision",target:"_blank",rel:"noopener noreferrer"}},[t._v("flow revisions"),e("OutboundLink")],1),t._v(" on our cluster. The last revision had 2.5K flows, but we don't see people creating a lot of revisions. Imagine 100K "),e("code",[t._v("byte[]")]),t._v(" to deserialize to POJO for every call. 🤯")]),t._v(" "),e("p",[t._v("Since we only need the last revision in our use case, we create an in-memory Map with all the flows using the following:")]),t._v(" "),e("div",{staticClass:"language-java extra-class"},[e("pre",{pre:!0,attrs:{class:"language-java"}},[e("code",[t._v("builder"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("addGlobalStore")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Stores")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("keyValueStoreBuilder")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Stores")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("persistentKeyValueStore")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("FLOW_STATE_STORE_NAME"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Flow")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    kafkaAdminService"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("getTopicName")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Flow")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Consumed")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Flow")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("withName")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"GlobalStore.Flow"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("GlobalInMemoryStateProcessor")]),e("span",{pre:!0,attrs:{class:"token generics"}},[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        FLOW_STATE_STORE_NAME"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        flows "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" kafkaFlowExecutor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setFlows")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("flows"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        store "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" kafkaFlowExecutor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("setStore")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("store"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("p",[e("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/streams/GlobalInMemoryStateProcessor.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("GlobalInMemoryStateProcessor"),e("OutboundLink")],1),t._v(" is a simple wrapper that saves the state store and sends a complete list on every change (not so frequent). Using this, we decided to gather all the flows in memory. This works well for our use cases because we know that an instance of Kestra will not have millions of flows.")]),t._v(" "),e("p",[t._v("Remember that all store operations (like get) will lead to deserialization that costs you some CPU.")]),t._v(" "),e("h2",{attrs:{id:"many-source-topics-within-a-kafka-stream"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#many-source-topics-within-a-kafka-stream"}},[t._v("#")]),t._v(" Many Source Topics Within a Kafka Stream")]),t._v(" "),e("p",[t._v("At first, we designed Kestra to have only one "),e("strong",[t._v("huge")]),t._v(" stream for all the processing of the executor. At first, it seemed cool, but this led to some drawbacks.")]),t._v(" "),e("p",[t._v("Here is the last version of our main and only Kafka Stream with many topics 🙉:"),e("br")]),e("figure",[e("img",{attrs:{src:"2023-02-23-techniques-kafka-streams-developer/topology.jpg",alt:"Kestra Topology",title:"Kestra Topology"}}),e("figcaption",[t._v("Kestra Topology")])]),e("br"),t._v("\nYes, this is a huge Kafka Stream. It was working well despite its complexity. But the major drawbacks were :"),e("p"),t._v(" "),e("ul",[e("li",[e("strong",[t._v("Monitoring")]),t._v(": All the metrics are under the same consumer group.")]),t._v(" "),e("li",[e("strong",[t._v("Debugging")]),t._v(": Each topic is consumed independently during a crash. When a message fails, the whole process crashes.")]),t._v(" "),e("li",[e("strong",[t._v("Lag")]),t._v(": This is the most important one. Since Kafka Streams optimize the consumption of messages by themselves, a topic with large outputs could lead to lag on unrelated topics. In that case, it is impossible to understand the lag on our consumers.")])]),t._v(" "),e("p",[t._v("Now, we have decided to split it into multiple "),e("a",{attrs:{href:"https://github.com/kestra-io/kestra/tree/92a856fa5b3fa3a29d251ec9873f631caa2678a0/runner-kafka/src/main/java/io/kestra/runner/kafka/executors",target:"_blank",rel:"noopener noreferrer"}},[t._v("streams"),e("OutboundLink")],1),t._v(" to be able to monitor and properly understand the lag on our Kafka Streams.")]),t._v(" "),e("p",[t._v("To split our giant stream, we dealt with only one topic at a time. We consumed only one topic at a time (to avoid large network transit), so we grouped all streams by source topics.")]),t._v(" "),e("h2",{attrs:{id:"do-more-with-kafka-streams"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#do-more-with-kafka-streams"}},[t._v("#")]),t._v(" Do More With Kafka Streams")]),t._v(" "),e("p",[t._v("We have covered some tips that took us a lot of time to find to deal with our issues. Even though there were some challenges, we could adapt our code so that Kafka worked well for our use case.")]),t._v(" "),e("p",[t._v("We learned how to use the same Kafka topic for source and destination, write a custom joiner for Kafka Streams, distribute workloads between multiple backends, use partitions to detect dead Kafka Consumers, tradeoffs for using state store "),e("code",[t._v("all()")]),t._v(", and using many source topics within a Kafka Stream.")]),t._v(" "),e("p",[t._v("We hope you enjoyed our story. Stay connected and follow "),e("a",{attrs:{href:"https://kestra.io",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kestra"),e("OutboundLink")],1),t._v(" on "),e("a",{attrs:{href:"https://github.com/kestra-io/kestra",target:"_blank",rel:"noopener noreferrer"}},[t._v("GitHub"),e("OutboundLink")],1),t._v(", "),e("a",{attrs:{href:"https://twitter.com/kestra_io",target:"_blank",rel:"noopener noreferrer"}},[t._v("Twitter"),e("OutboundLink")],1),t._v(", or "),e("a",{attrs:{href:"https://api.kestra.io/v1/communities/slack/redirect",target:"_blank",rel:"noopener noreferrer"}},[t._v("Slack"),e("OutboundLink")],1),t._v(".")])])}),[],!1,null,null,null);a.default=n.exports}}]);