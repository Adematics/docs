(window.webpackJsonp=window.webpackJsonp||[]).push([[37],{639:function(t,e,a){t.exports=a.p+"assets/img/1.84d5b594.png"},640:function(t,e,a){t.exports=a.p+"assets/img/2.1b969feb.png"},743:function(t,e,a){"use strict";a.r(e);var s=a(19),n=Object(s.a)({},(function(){var t=this,e=t.$createElement,s=t._self._c||e;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("The Google Cloud Platform (GCP) is a widely used cloud platform to build an end-to-end solution for data pipeline starting from collecting the data in the Data warehouse to building and hosting scalable applications.")]),t._v(" "),s("p",[t._v("BigQuery is generally used to store and analyse structured Big data in GCP and also as a main tool for the below use-cases in GCP:")]),t._v(" "),s("ul",[s("li",[t._v("BigQuery is often used as a "),s("strong",[t._v("Data Warehouse")]),t._v(" with serverless query operations for data analysis.")]),t._v(" "),s("li",[t._v("It offers "),s("strong",[t._v("data transfer services")]),t._v(" to transfer the data tables to/from BigQuery to another cloud platform.")]),t._v(" "),s("li",[t._v("There are "),s("strong",[t._v("SDKs")]),t._v(" in multiple language-specific and "),s("strong",[t._v("gcloud CLI")]),t._v(" available to interact with BigQuery")]),t._v(" "),s("li",[t._v("It has features for ML model "),s("strong",[t._v("training")]),t._v(" and "),s("strong",[t._v("prediction")]),t._v(" on regular intervals using scheduled BigQuery ML queries")])]),t._v(" "),s("p",[t._v("Though GCP provides tools like the gcloud CLI and Scheduled Query for simple data processing, these canâ€™t be used to connect multiple data sources and destinations, to create data modelization along with creating the dependency structure, and visualization of each task. For such complex data pipelines, we would need an orchestration tool like Kestra.")]),t._v(" "),s("h3",{attrs:{id:"what-is-kestra"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#what-is-kestra"}},[t._v("#")]),t._v(" What is Kestra?")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://github.com/kestra-io/kestra",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("Kestra")]),s("OutboundLink")],1),t._v(" is an "),s("strong",[t._v("open-source")]),t._v(" and "),s("strong",[t._v("cloud-native")]),t._v(" tool that can "),s("strong",[t._v("scale infinitely")]),t._v(" and serves as a "),s("strong",[t._v("low code")]),t._v(" data orchestrator and dependency generator to create and schedule the data flow from multiple sources and destinations. It provides an elegant visualization of the entire DAG including all the tasks and plugins used in it. There are multiple "),s("a",{attrs:{href:"https://kestra.io/plugins/",target:"_blank",rel:"noopener noreferrer"}},[t._v("plugins"),s("OutboundLink")],1),t._v(" available for many cloud platforms like GCP, AWS, and Azure to implement complex pipelines.")]),t._v(" "),s("p",[t._v("For Google Cloud, Kestra has an entire range of plugins for various services like GCS, BigQuery, VertexAI, etc. More specifically there are plugins for BigQuery used to create the ETL/ELT pipeline to any other services that are readily available in Kestra.")]),t._v(" "),s("p",[t._v("We can create a flow to execute these operations using YAML language which would require minimum user inputs like the task name, type, and inputs configuration. This blog also has a simple tutorial that covers the most basic BigQuery operations using Kestra.")]),t._v(" "),s("h3",{attrs:{id:"kestra-integrates-widely-with-bigquery"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#kestra-integrates-widely-with-bigquery"}},[t._v("#")]),t._v(" Kestra integrates widely with BigQuery")]),t._v(" "),s("p",[t._v("Kestra cover all the standard operations in BigQuery like creating and deleting the dataset and table, running the query, copying and loading the table, and importing/exporting the table to/from BigQuery to GCS. All these can be done using the BigQuery plugins.")]),t._v(" "),s("h3",{attrs:{id:"data-modelization"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-modelization"}},[t._v("#")]),t._v(" Data Modelization")]),t._v(" "),s("p",[t._v("The Data Modelization refers to creating a visual representation of data flow between data points and structures. This can be achieved by applying a sequence of transformation or aggregation queries to the raw dataset and using the final data used for visualization, analysis or machine learning. For example, To apply complex aggregation on daily sales report data and use those data points in subsequent phases of transformations for gathering the daily sales trends.")]),t._v(" "),s("p",[t._v("The "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.Query.html",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("Query")]),s("OutboundLink")],1),t._v(" plugin for BigQuery in Kestra is commonly used to achieve Data modelization by running the query on the table stored in BigQuery and applying any further transformation or aggregation using the SQL query. To implement data modelization in a data pipeline, the query result needed to be stored in the BigQuery table. Using the "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.Query.html",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("Query")]),s("OutboundLink")],1),t._v(" plugin and by providing the "),s("code",[t._v("destinationTable")]),t._v(" in schema input, the result will be stored and can be used in the next phases.")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" query\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.plugin.gcp.bigquery.Query\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("destinationTable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" kestra"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("dev.ETL_demo.analysis_data\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("writeDisposition")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" WRITE_APPEND\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("sql")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("|")]),s("span",{pre:!0,attrs:{class:"token scalar string"}},[t._v('\n  SELECT Sex, Age, COUNT(Survived) Survived_users, "{{ execution.id }}" as lineage_cols\n  FROM `kestra-dev.ETL_demo.raw_data`\n  GROUP BY 1,2\n  ORDER BY 3 DESC')]),t._v("\n")])])]),s("p",[t._v("To generate the daily sales report on a day to day basis, the frequency of flow should be set to one day. To eliminate the manual triggering, Kestra offers scheduling a flow where the cron setting is specified in the YAML or else in the schedule UI. The automated flow will ensure all the data modelization pipeline runs smoothly and generates insights every day.")]),t._v(" "),s("p",[t._v("Here is a sample trigger for scheduling the flow:")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("triggers")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" schedule\n   "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.core.models.triggers.types.Schedule\n   "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("cron")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"0 0 * * *"')]),t._v("\n")])])]),s("p",[t._v("The "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.Query.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Query"),s("OutboundLink")],1),t._v(" plugin also allows the fetch parameters in order to use the output of a SQL query to be used on the next tasks. A common usage can be to fetch the max date currently on a table and to use it on a later query.")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tasks")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" query\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.plugin.gcp.bigquery.Query\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("fetchOne")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean important"}},[t._v("true")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("sql")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("|")]),s("span",{pre:!0,attrs:{class:"token scalar string"}},[t._v("\n    SELECT MAX(added_date) AS date\n    FROM `kestra-dev.ETL_demo.raw_data`")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" query\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.plugin.gcp.bigquery.Query\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("fetchOne")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean important"}},[t._v("true")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("destinationTable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" kestra"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("dev.ETL_demo.destination\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("writeDisposition")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" WRITE_APPEND\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("sql")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("|")]),s("span",{pre:!0,attrs:{class:"token scalar string"}},[t._v("\n    SELECT *\n    FROM `kestra-dev.ETL_demo.source`\n    WHERE added_date > date('{{ outputs.query.row.date }}')")]),t._v("\n")])])]),s("h3",{attrs:{id:"data-lineage"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#data-lineage"}},[t._v("#")]),t._v(" Data Lineage")]),t._v(" "),s("p",[t._v("The Flow (DAG) executions can be analysed using the Execution section of Kestra. This section contains the list of all executions with details on how the flow is triggered (Manually or using the above approach) and an "),s("strong",[t._v("Execution Id")]),t._v(" is also assigned to every run. Each execution contains the various Data Lineage views like Gantt chart, Logs, Topology, and Outputs.")]),t._v(" "),s("p",[t._v("In the "),s("strong",[t._v("Gantt chart")]),t._v(", we can see relevant information about the time taken for the data processing of each task in the Flow.")]),t._v(" "),s("p",{staticStyle:{"text-align":"center"}},[s("img",{staticClass:"rounded img-thumbnail mt-4 mb-4",attrs:{src:a(639),alt:"Data execution in Kestra"}})]),t._v(" "),s("p",[t._v("In "),s("strong",[t._v("Topology")]),t._v(", a sequence of tasks along with the dependencies present in the Flow can be visualised by the user to debug the tasks.")]),t._v(" "),s("p",{staticStyle:{"text-align":"center"}},[s("img",{staticClass:"rounded img-thumbnail mt-4 mb-4",attrs:{src:a(640),alt:"Data lineage in Kestra"}})]),t._v(" "),s("p",[t._v("All the necessary information about the flow can be accessed using the execution id. Resulting in providing Data Lineage on the flow. Also, while creating the Flow "),s("code",[t._v("executionId")]),t._v(" can be accessible by "),s("code",{pre:!0},[t._v("{{ execution.id }}")]),t._v(" variable by which the user can add any execution identifier in the flow to separate out other executions. We can also inject an "),s("code",{pre:!0},[t._v("{{ execution.id }}")]),t._v(" variable in the SQL query that allows us to track the execution from where the data came from.")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" Sex"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Age"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("COUNT")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Survived"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" Survived_users"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{{ execution.id }}"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" lineage_cols\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token identifier"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")]),t._v("kestra-dev.ETL_demo.raw_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("`")])]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ORDER")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DESC")]),t._v("\n")])])]),s("h3",{attrs:{id:"interact-with-google-cloud-storage"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#interact-with-google-cloud-storage"}},[t._v("#")]),t._v(" Interact with Google Cloud Storage")]),t._v(" "),s("p",[t._v("The "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.LoadFromGcs.html",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("LoadFromGcs")]),s("OutboundLink")],1),t._v(" plugin is used to import the data from GCS and store it in the BigQuery table directly. This can be especially helpful to analyse and generate insights from the static data files stored in GCS. This plugin can take the input data files for "),s("strong",[t._v("various file formats")]),t._v(" like Avro, JSON, PARQUET, ORC, and CSV.")]),t._v(" "),s("p",[t._v("A sample flow to load the data from GCS and store it in a BigQuery table with specified inputs.")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" load_from_gcs\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.plugin.gcp.bigquery.LoadFromGcs\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("destinationTable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" ETL_demo.raw_data\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("ignoreUnknownValues")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean important"}},[t._v("true")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("schema")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("fields")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Survived\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" STRING\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Sex\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" STRING\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Age\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" STRING\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" CSV\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("csvOptions")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("allowJaggedRows")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean important"}},[t._v("true")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("encoding")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" UTF"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("fieldDelimiter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('","')]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("from")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" gs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("//sandbox"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("kestra"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("dev/sandbox/titanic.csv\n")])])]),s("p",[t._v("On the other side, "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.ExtractToGcs.html",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("ExportToGCS")]),s("OutboundLink")],1),t._v(" plugin is used to extract the table from BigQuery and store the table as per the specified path to the GCS bucket. This would be useful in the use-cases where a BigQuery table needs to be utilized in other services/platforms as part of the entire solution. Example: We may need to have a backup of the table in GCS to save the storage cost in BigQuery or even create a dataset file to train ML models.")]),t._v(" "),s("p",[t._v("Below is an example of a simple flow to upload the data back to GCS as a specific destination path.")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" export_to_gcs\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.plugin.gcp.bigquery.ExtractToGcs\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("destinationUris")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" gs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("//sandbox"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("kestra"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("dev/sandbox/"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" inputs.destinationFile "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(".csv\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("sourceTable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" kestra"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("dev.ETL_demo.analysis_data\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" CSV\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("fieldDelimiter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" ;\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("printHeader")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean important"}},[t._v("true")]),t._v("\n")])])]),s("p",[t._v("While executing the Flow in Kestra, the inputs can be provided in UI or Curl. For instance, to execute the flow, below are the inputs required.")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("inputs")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" destinationFile\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" STRING\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("required")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean important"}},[t._v("true")]),t._v("\n")])])]),s("p",[t._v("The CURL command (complete "),s("a",{attrs:{href:"https://kestra.io/docs/api-guide/",target:"_blank",rel:"noopener noreferrer"}},[t._v("API"),s("OutboundLink")],1),t._v(") can also be used to trigger the flow if you need to automatize the execution from another application. Here is a sample CURL for such a use case.")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[s("span",{pre:!0,attrs:{class:"token function"}},[t._v("curl")]),t._v(" -v "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"<http://localhost:8080/api/v1/executions/trigger/io.kestra.gcp/extract-to-gcs>"')]),t._v("\n    -H "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Transfer-Encoding:chunked"')]),t._v("\n    -H "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Content-Type:multipart/form-data"')]),t._v("\n    -F "),s("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("destinationFile")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"analysis_data_18"')]),t._v("\n")])])]),s("h3",{attrs:{id:"storage-write"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#storage-write"}},[t._v("#")]),t._v(" Storage Write")]),t._v(" "),s("p",[t._v("All these operations can be done if the data is stored in GCP platform services. But what if the data is stored in external servers like Database, NoSQL, Queue or any other plugins providing Kestraâ€™s internal server files. Kestra offers a service to import data from other servers into BigQuery using the "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.StorageWrite.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("StorageWrite"),s("OutboundLink")],1),t._v(" plugin that will use the "),s("a",{attrs:{href:"https://cloud.google.com/bigquery/docs/write-api",target:"_blank",rel:"noopener noreferrer"}},[t._v("Storage Write API"),s("OutboundLink")],1),t._v(" from BigQuery. Another advantage of using this plugin is to avoid quotas limitation to ingest the data while data streaming in real-time or in batch job writing.")]),t._v(" "),s("p",[t._v("Refer below for an example flow that will stream from a Kafka topic to a BigQuery table using StorageWrite api, really useful to achieve near real time without hitting BigQuery limits.")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tasks")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" consume\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.plugin.kafka.Consume\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("topic")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" TRANSACTIONS"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("LOG"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("V1\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("properties")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("bootstrap.servers")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"<CONFLUENT-URI>.gcp.confluent.cloud:9092"')]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("security.protocol")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" SASL_SSL\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("sasl.mechanism")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" PLAIN\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("sasl.jaas.config")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(' org.apache.kafka.common.security.plain.PlainLoginModule required username="<USERNAME'),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")]),t._v('" password="<PASSWORD'),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")]),t._v('";\n    '),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("serdeProperties")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("schema.registry.url")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("//<CONFLUENT"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("URI"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")]),t._v(".aws.confluent.cloud\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("basic.auth.credentials.source")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" USER_INFO\n      "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("basic.auth.user.info")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" <USERNAME"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("PASSWORD"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("keyDeserializer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" STRING\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("valueDeserializer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" AVRO\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"storage_write"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"io.kestra.plugin.gcp.bigquery.StorageWrite"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("bufferSize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("from")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{{ outputs.transform.uri }}"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("destinationTable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"kestra-dev.ETL_demo.transactions_logs"')]),t._v("\n")])])]),s("h3",{attrs:{id:"react-to-event"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#react-to-event"}},[t._v("#")]),t._v(" React to event")]),t._v(" "),s("p",[t._v("Using the above scheduling features in data modelization, we can trigger the flow in a predefined time. Kestra has a lot of advanced features like triggering the flow when a file is uploaded in GCS or in a BigQuery table. There would be a scenario where the data pipeline needs to be started whenever a new table is ingested in BigQuery or archive the file in a different folder of GCS when any file is uploaded.")]),t._v(" "),s("p",[t._v("Kestra has two trigger plugins for both BigQuery and GCS:")]),t._v(" "),s("ul",[s("li",[t._v("BigQuery Trigger")]),t._v(" "),s("li",[t._v("GCS Trigger")])]),t._v(" "),s("p",[t._v("The BigQuery "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/triggers/bigquery/io.kestra.plugin.gcp.bigquery.Trigger.html",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("Trigger")]),s("OutboundLink")],1),t._v(" will check for the data arriving in the BigQuery table based on a Query and will invoke a flow with loop ("),s("a",{attrs:{href:"https://kestra.io/plugins/core/tasks/flows/io.kestra.core.tasks.flows.EachSequential.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("EachSequential"),s("OutboundLink")],1),t._v(" task) for each row. The below YAML can be used to create such a Flow in Kestra.")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Trigger_flow\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("namespace")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" com.kestra.sandbox\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("revision")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tasks")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" each\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.core.tasks.flows.EachSequential\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tasks")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" return\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.core.tasks.debugs.Return\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{{taskrun.value}}"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("value")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{{ trigger.rows }}"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("triggers")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" watch\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.plugin.gcp.bigquery.Trigger\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("interval")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" PT30S\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("sql")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" SELECT * FROM `kestra"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("dev.ETL_demo.raw_data`\n")])])]),s("p",[t._v("We can also create a Flow that will trigger when a new file is uploaded to a specified folder of GCS and as an action, it will move the file to the archive folder.")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" gcs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("listen\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("namespace")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.sandbox\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tasks")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" each\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.core.tasks.flows.EachSequential\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tasks")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" return\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.core.tasks.debugs.Return\n        "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{{taskrun.value}}"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("value")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("\"{{ trigger.blobs | jq '[].uri' }}\"")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("triggers")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("id")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" watch\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" io.kestra.plugin.gcp.gcs.Trigger\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("interval")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"PT5M"')]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("from")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" gs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("//my"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("bucket/kestra/listen/\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("action")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" MOVE\n    "),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("moveDirectory")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" gs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("//my"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("bucket/kestra/archive/\n")])])]),s("p",[t._v("Here the "),s("strong",[t._v("interval")]),t._v(" in both flows means a time gap between subsequent calls to check for the data in order to avoid reaching Google Cloud limits.")]),t._v(" "),s("h3",{attrs:{id:"complete-etl-pipeline"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#complete-etl-pipeline"}},[t._v("#")]),t._v(" Complete ETL pipeline")]),t._v(" "),s("p",[t._v("For the use cases where multiple tasks need to be run in parallel and we would need to create an internal dependency between those tasks. For this, we can use the Kestra features and plugins to maintain the pipeline orchestration, data modelization and data lineage.")]),t._v(" "),s("p",[t._v("For instance, we want to build a pipeline where we want to find out daily available stock based on the previous dayâ€™s sales and stock data. We can create the pipeline using the above basic Kestra operations and plugin features and this pipeline could consist of the following tasks:")]),t._v(" "),s("ol",[s("li",[s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/triggers/gcs/io.kestra.plugin.gcp.gcs.Trigger.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Setup a trigger"),s("OutboundLink")],1),t._v(" which will invoke the pipeline when daily sales and stock data are dumped in the GCS bucket.")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.CreateDataset.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Creating a dataset"),s("OutboundLink")],1),t._v(" in BigQuery specifically to store all the intermediate tables.")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.LoadFromGcs.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Loading the data from the Google Cloud Storage"),s("OutboundLink")],1),t._v(" and storing it in the destination tables.")]),t._v(" "),s("li",[t._v("Run "),s("a",{attrs:{href:"https://kestra.io/plugins/core/tasks/flows/io.kestra.core.tasks.flows.Parallel.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("parallel"),s("OutboundLink")],1),t._v(" tasks to aggregate the product level stock and sales data and store both tables in BigQuery.")]),t._v(" "),s("li",[t._v("Run a "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.Query.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("query"),s("OutboundLink")],1),t._v(" to calculate the remaining stock at the product level by subtracting from stock to sales only after storing the above tables.")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.ExtractToGcs.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Export in Google Cloud Storage"),s("OutboundLink")],1),t._v(" the updated stock data.")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.DeleteTable.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Clean up all the intermediate tables"),s("OutboundLink")],1),t._v(" and "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.DeleteDataset.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("datasets in BigQuery"),s("OutboundLink")],1),t._v(".")])]),t._v(" "),s("p",[t._v("To trigger the flow, we can set the "),s("a",{attrs:{href:"https://kestra.io/plugins/plugin-gcp/triggers/gcs/io.kestra.plugin.gcp.gcs.Trigger.html",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("GCS")]),s("OutboundLink")],1),t._v(" "),s("strong",[t._v("Trigger")]),t._v(" plugin where we can provide the condition to trigger the flow only if the file should be present in the listening folder of the GCS bucket.")]),t._v(" "),s("p",[t._v("For creating a sequential execution of tasks that has the dependency on the previous task, we need to provide all the sub-tasks in the parent task of the "),s("a",{attrs:{href:"https://kestra.io/plugins/core/tasks/flows/io.kestra.core.tasks.flows.Sequential.html",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("Sequential")]),s("OutboundLink")],1),t._v(" type. This sequential flow will be used to calculate the remaining stock at the product level using a Query plugin.")]),t._v(" "),s("p",[t._v("While to run the tasks in parallel where the output of the task is not dependent, we need to specify all these tasks in type as "),s("a",{attrs:{href:"https://kestra.io/plugins/core/tasks/flows/io.kestra.core.tasks.flows.Parallel.html",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("Parallel")]),s("OutboundLink")],1),t._v(". In the above example, aggregating sales and stock data at the product level are two independent tasks which can be run in parallel using this plugin type.")]),t._v(" "),s("p",[t._v("Here we can also provide the output of one task to the input to another task using the "),s("code",{pre:!0},[t._v("{{outputs.task_id.output_parameter}}")]),t._v("`.")]),t._v(" "),s("h3",{attrs:{id:"conclusion"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[t._v("#")]),t._v(" Conclusion")]),t._v(" "),s("p",[t._v("We saw that "),s("a",{attrs:{href:"https://www.kestra.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kestra"),s("OutboundLink")],1),t._v(" offers a variety of plugins "),s("strong",[t._v("for creating a complete ETL/ELT pipeline using the GCP BigQuery")]),t._v(" service along with features for monitoring the pipeline executions. It can be also used to schedule the Flow, provide easy debugging, and maintain the external dependencies. This blog post tries to showcase a few uses-case of how Kestra can be used along with the sample snippets for using the Kestra hands-on for you to start exploring the "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kestra"),s("OutboundLink")],1),t._v(" as it can cover endless use-cases.")])])}),[],!1,null,null,null);e.default=n.exports}}]);