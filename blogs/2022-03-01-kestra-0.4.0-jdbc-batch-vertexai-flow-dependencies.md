---
title: "Kestra 0.4 release and new plugins"
description: Discover how Leroy Merlin moved all their data pipelines to Google Cloud with Kestra
date: 2022-03-01T10:00:00
layout: BlogsPost
author:
  name: Ludovic Dehon
  image: "ldehon"
  twitter: "@tchiotludo"
image: /blogs/2022-03-01-kestra-0.4.0.jpg
draft: true
---

Since our [public launch](./2022-02-01-kestra-opensource), we have made a lot of works to give you the better experience we expect with Kestra. This version brings a lot of performance enhancement in order to provide the smooth experience with large clusters and some others cool features.


## Performance for large clusters

Since we have already a [large deployment](./2022-02-22-leroy-merlin-usage-kestra) at Leroy Merlin, we have hit many times some performance issue, but this one was more complicated to find. Here you will expose you some metrics based on our large deployment on production environment for [Leroy Merlin](./2022-02-22-leroy-merlin-usage-kestra) and will show you charts before and after the changes for the same workload. Leroy Merlin usage is mostly processed during the night, all flows starting at the same time near 3AM.



We have made a hard work **reduce cpu usage and latency**.

As you can see, the same workload use 3 executors before (we used autoscaling) before, with a total cpu usage of 2.5 cores for more than 6 hours.
After, we are mostly **used 0.5 cores** with a peek at 1.5 core only for 1 hours, all the workload handle by only 2 executors (minimum of autoscaling).

<figure class="figure" style="width: 48%">
  <img class="figure-img img-fluid rounded" src="./2022-03-01-kestra-0.4.0/executor-cpu-before.png" alt="before" />
  <figcaption class="figure-caption text-center">Before</figcaption>
</figure>

<figure class="figure" style="width: 48%">
  <img class="figure-img img-fluid rounded" src="./2022-03-01-kestra-0.4.0/executor-cpu-after.png" alt="after" />
  <figcaption class="figure-caption text-center">After</figcaption>
</figure>



For the kafka side, we have also **improved the latency**.
We have a lot of [flow triggers](/docs/developer-guide/triggers/flow.html), that need to be analysed for each execution ended. When you have high volume execution, a lot of message is send through Kafka and if your consumer is too slow, the queue is filling and increase latency.
Before, we have a large accumulation of message that lead to late start of flow executions (few minutes). The optimisation let us keep flow start few seconds after.


<figure class="figure" style="width: 48%">
  <img class="figure-img img-fluid rounded" src="./2022-03-01-kestra-0.4.0/lag-before.png" alt="before" />
  <figcaption class="figure-caption text-center">Before</figcaption>
</figure>

<figure class="figure" style="width: 48%">
  <img class="figure-img img-fluid rounded" src="./2022-03-01-kestra-0.4.0/lag-after.png" alt="after" />
  <figcaption class="figure-caption text-center">After</figcaption>
</figure>

The last charts is showing the delay between the task creation and the task started by worker. As we optimized the Kafka processing globally, we avoid to enqueue message on kafka and reduce the delay between creation of the task and processing by workers.

<figure class="figure" style="width: 48%">
  <img class="figure-img img-fluid rounded" src="./2022-03-01-kestra-0.4.0/queue-before.png" alt="before" />
  <figcaption class="figure-caption text-center">Before</figcaption>
</figure>

<figure class="figure" style="width: 48%">
  <img class="figure-img img-fluid rounded" src="./2022-03-01-kestra-0.4.0/queue-after.png" alt="after" />
  <figcaption class="figure-caption text-center">After</figcaption>
</figure>

All these improvements also offer a **large reduction on total duration** of execution.  All message being consumed quickly, the delay between each task is reduced and will reduce the total duration of the execution.

<figure class="figure" style="width: 48%">
  <img class="figure-img img-fluid rounded" src="./2022-03-01-kestra-0.4.0/executor-duration-before.png" alt="before" />
  <figcaption class="figure-caption text-center">Before</figcaption>
</figure>

<figure class="figure" style="width: 48%">
  <img class="figure-img img-fluid rounded" src="./2022-03-01-kestra-0.4.0/executor-duration-after.png" alt="after" />
  <figcaption class="figure-caption text-center">After</figcaption>
</figure>

Keep in mind, that Leroy Merlin workflow is unbalanced, and all the executions is staring at the same time, with 3000+ executions & 35,000+ tasks in a short time, 50% of the workload of the whole day. We need to provide the same exigence for night processing even if we are all sleep.
We have some other optimizations in the backlog, but we have made a great improvement right now that will handle more complex cluster with a lot of flow & concurrent executions. A full blog post is coming to expose what we have discovered scaling a [Kafka Streams](https://kafka.apache.org/documentation/streams/) application.

## Resilience
We rely on kestra internal storage for data passed between tasks,


## New plugins & improvement

### JDBC plugins

#### Batch query
Jdbc plugins have a major update allowing to bulk request. This allows you to use any Kestra storage generated files in order to generate a batch request.
Now you can read data from any task and generate a bulk request in order to insert or update data on most jdbc database.
Look at this example:

```yaml
tasks:
  - id: query
    type: "io.kestra.plugin.jdbc.mysql.Query"
    url: jdbc:mysql://127.0.0.1:56982/
    username: mysql_user
    password: mysql_passwd
    sql: select * from users
    store: true
  - id: load
    type: "io.kestra.plugin.jdbc.sqlserver.Batch"
    url: jdbc:sqlserver://localhost:41433;trustServerCertificate=true
    username: sa
    password: Sqls3rv3r_Pa55word!
    from: "{{ inputs.query.uri }}"
    sql: "insert into users values( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )"
```
We read a from a mysql database table using a [Query](/plugins/plugin-jdbc-mysql/tasks/io.kestra.plugin.jdbc.mysql.Query.html) and store it on internal storage, after that, we generate a [bulk insert](/plugins/plugin-jdbc-sqlserver/tasks/io.kestra.plugin.jdbc.sqlserver.Batch.html) that will load the resulting dataset to a Microsoft SQL Server database.

::: success Move data to jdbc easily
As we rely on internal storage of Kestra, any task that produce internal kestra storage like [JsonReader](/plugins/plugin-serdes/tasks/json/io.kestra.plugin.serdes.json.JsonReader.html), [AvroReader](/plugins/plugin-serdes/tasks/avro/io.kestra.plugin.serdes.avro.AvroReader.html), ... can be used as source. You can now move data from any source and any format thanks to Kestra plugins.
:::

#### New Jdbc plugin
We also introduce 2 new jdbc plugins :
- [Microsoft SQL Server](/plugins/plugin-jdbc-sqlserver/)
- [Actian Vectorwise](/plugins/plugin-jdbc-vectorwise/)

Both support Query & Batch query enabling you to imagine a lot more use cases.

## Kafka
Kafka plugins was also release with a first task [Produce](/plugins/plugin-kafka/tasks/io.kestra.plugin.kafka.Produce.html) (Consume will come soon). Like other plugins jdbc, json, csv, ..., Kafka plugins rely on Kestra internal storage, allowing you to send from any source to Kafka.

We support for now many [types of serializer inside Kafka](/plugins/plugin-kafka/tasks/io.kestra.plugin.kafka.Produce.html#valueserializer) but most notable are `STRING`, `JSON` & `AVRO` with support with [Kafka schema registry](https://docs.confluent.io/platform/current/schema-registry/index.html).

This is just the beginning for this plugins but plan to support Json schema & Protobuf with schema registry. Also, as we said before, we want to support a `Consume` tasks (with ou without of consumer groups). Maybe also a [Trigger](/docs/developer-guide/triggers/) based on `Consume` that allow you start executions based on incoming topic, we wait for more community feedback on this part.


## Singer plugins
We made an evolution on how [singer plugins](/plugins/plugin-singer/). Singer have 2 concepts: taps (source of data) and targets (destination, where you load data). This pattern is smart since you can have a lot of different sources that can be load on as many destinations that you need thanks to [singer specifications](https://github.com/singer-io/getting-started/blob/master/docs/SPEC.md).

Before the plugins have a single task target that embed a tap that allow you to load from a source to 1 destination only. Now we have 2 different tasks that allow you to download 1 time from a tap and send the same result to many destinations.

Here is an example loading [GitHub](/plugins/plugin-singer/tasks/taps/io.kestra.plugin.singer.taps.GitHub.html) from a repository to a [BigQuery Dataset](/plugins/plugin-singer/tasks/targets/io.kestra.plugin.singer.targets.AdswerveBigQuery.html):

```yaml
tasks:
  - id: github
    type: io.kestra.plugin.singer.taps.GitHub
    accessToken: "{{ vars.github.token }}"
    repositories:
      - kestra-io/kestra
    startDate: "2019-07-01"
    raw: true
    streamsConfigurations:
      - replicationMethod: INCREMENTAL
        selected: true
      - selected: false
        stream: projects
      - selected: false
        stream: project_cards
      - selected: false
        stream: project_columns
    runner: DOCKER
    dockerOptions:
      image: python:3.8
  - id: bigquery
    type: io.kestra.plugin.singer.targets.AdswerveBigQuery
    addMetadataColumns: true
    from: "{{ outputs.github.raw }}"
    datasetId: github
    location: EU
    projectId: "{{vars.serviceAccount}}"
    serviceAccount: "{{vars.projectId}}"
    runner: DOCKER
    dockerOptions:
      image: python:3.8
```

But you can still use Kestra internal storage with any singer taps and use the data with any Kestra tasks:

```yaml
  - id: github
    # same as above
    raw: false
  - id: update
    type: "io.kestra.plugin.jdbc.sqlserver.Batch"
    url: jdbc:sqlserver://localhost:41433;trustServerCertificate=true
    username: sa
    password: Sqls3rv3r_Pa55word!
    from: "{{ outputs.github.streams.commit }}"
    sql: "insert into commit values( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )"


```

We also add another singer destination [Oracle](/plugins/plugin-singer/tasks/taps/io.kestra.plugin.singer.taps.PipelinewiseOracle.html).


## GCP

### Vertex AI Custom Job

[VertexAI](https://cloud.google.com/vertex-ai) is a full suite for Machine Learning allowing you to build, deploy, and scale ML models faster.

We have added a tasks [CustomJob](/plugins/plugin-gcp/tasks/vertexai/io.kestra.plugin.gcp.vertexai.CustomJob.html) that Start a Vertex AI [custom job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). This one is based on a docker image that you can launch on any instance type, with or without GPU. It enables any kinds of custom code to deployed in order to be run in an ephemeral clusters and will be stop at the end of job. Perfect for large training of machine learning, but it can be any docker image that need to a large compute engine without having to create Kubernetes cluster or Compute engine.

The integration will start the vertex job and will wait till the end of the job in order to transmit the status of the job to Kestra. We had made a deep integration, so you will also received logs in realtime for your running jobs.

```yaml
tasks:
  - id: tableAnalysis
    type: io.kestra.plugin.gcp.vertexai.CustomJob
    delete: false
    displayName: "{{ task.id }}"
    projectId: "{{vars.serviceAccount}}"
    serviceAccount: "{{vars.projectId}}"
    region: europe-west1
    spec:
      serviceAccount: service-account-name@project-name.iam.gserviceaccount.com
      workerPoolSpecs:
        - containerSpec:
            args:
              - "-e"
              - "{{ globals.env }}"
            commands:
              - /app/start.sh
            imageUri: "{{vars.imageUri}}"
          machineSpec:
            machineType: n1-standard-4
          replicaCount: 1

```

### BigQuery retry
We also have improved the retry on All BigQuery tasks. By default, we retry all operations with an internal errors for Google servers, but also [some errors](/plugins/plugin-gcp/tasks/bigquery/io.kestra.plugin.gcp.bigquery.Query.html#retrymessages) that could happen in real life: `rateLimitExceeded`, `due to concurrent update`, ... are many cases that a simple retry will make the tasks success. So we enable it by default. On a large BigQuery usage like the Leroy Merlin, this avoids unexpected failure that a simple retry could solve.

Now, we must catch all errors on BigQuery that can be retried.

## Flow dependencies in Entreprise edition
For the entreprise edition, we have delivered the most wanted features: ability to see all flow dependencies recursively.

<video controls="true" allowfullscreen="true">
  <source src="./2022-03-01-kestra-0.4.0/deps.mp4" type="video/mp4">
</video>

We also have added a confirmation when deleting a flow that have a dependencies, that warn user that the deletion could break the whole production plan.

<p class="text-center">
  <img src="./2022-03-01-kestra-0.4.0/deps.png" class="rounded img-thumbnail" alt="Kestra user interface">
</p>
<div class="clearfix" />

This is high value feature that enable to have a full vision on all your data pipeline across teams. With many one consuming data from others one, no one can be sure if this flow changed, no others flow will be impacted. Impact analysis is greatly simplify thanks to this strong ui.


